"""
å‘¨çº¿çº§åˆ«è‚¡ç¥¨ç­›é€‰å™¨
- æœˆçº¿å¤§åŠ¿åˆ¤å®šï¼ˆä¸Šå‡è¶‹åŠ¿/åº•éƒ¨ç­‘åº•/ä¸‹è·Œè¶‹åŠ¿ï¼‰
- å‘¨çº¿ç»“æ„éªŒè¯ï¼ˆå‡çº¿/åŠ¨èƒ½/å½¢æ€ï¼‰
- ç”Ÿæˆé‡ç‚¹è§‚å¯Ÿè‚¡ç¥¨æ± 

ä½¿ç”¨æ–¹æ³•:
    python stock_all/weekly_scan.py --config stock_all/config.yaml
"""

from __future__ import annotations

import argparse
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
import yaml
from tqdm import tqdm

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from indicators import (
    analyze_volume_pattern,
    calculate_all_indicators,
    calculate_ma_slope,
    check_ma_alignment,
    detect_macd_golden_cross,
)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="å‘¨çº¿çº§åˆ«è‚¡ç¥¨ç­›é€‰å™¨")
    parser.add_argument(
        "--config",
        default="stock_all/config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--data-dir",
        default=None,
        help="Kçº¿æ•°æ®ç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰",
    )
    return parser.parse_args()


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def load_kline_data(stock_code: str, data_dir: Path, freq: str) -> Optional[pd.DataFrame]:
    """
    åŠ è½½Kçº¿æ•°æ®
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ 600000ï¼‰
        data_dir: æ•°æ®ç›®å½•
        freq: é¢‘ç‡ 'daily', 'weekly', 'monthly'
        
    Returns:
        Kçº¿æ•°æ®DataFrameæˆ–None
    """
    # ç¡®ä¿stock_codeæ˜¯å­—ç¬¦ä¸²ç±»å‹
    stock_code = str(stock_code)
    
    freq_map = {
        'daily': '_daily_1y.csv',
        'weekly': '_weekly_5y.csv',
        'monthly': '_monthly_10y.csv'
    }
    
    file_path = data_dir / stock_code / f"{stock_code}{freq_map[freq]}"
    
    if not file_path.exists():
        return None
    
    try:
        df = pd.read_csv(file_path)
        if df.empty:
            return None
        
        # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
        df = df.sort_values('date').reset_index(drop=True)
        return df
    except Exception as e:
        print(f"åŠ è½½ {file_path} å¤±è´¥: {e}")
        return None


def pre_filter_stock(stock_code: str, daily_df: pd.DataFrame, config: dict) -> tuple[bool, str]:
    """
    åŸºç¡€é¢„è¿‡æ»¤
    
    Returns:
        (æ˜¯å¦é€šè¿‡, å¤±è´¥åŸå› )
    """
    params = config['pre_filter']
    
    # æ£€æŸ¥STè‚¡ç¥¨
    if params['exclude_st'] and 'isST' in daily_df.columns:
        if daily_df['isST'].iloc[-1] == '1':
            return False, "STè‚¡ç¥¨"
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    if len(daily_df) < 60:  # è‡³å°‘60ä¸ªäº¤æ˜“æ—¥
        return False, "æ•°æ®ä¸è¶³"
    
    # æ£€æŸ¥åœç‰Œï¼ˆæœ€è¿‘5æ—¥ï¼‰
    recent_5d = daily_df.tail(5)
    if 'tradestatus' in recent_5d.columns:
        suspend_days = (recent_5d['tradestatus'] == '0').sum()
        if suspend_days > params['max_suspend_days_5d']:
            return False, f"åœç‰Œå¤©æ•°è¿‡å¤š({suspend_days})"
    
    # æ£€æŸ¥20æ—¥å‡æˆäº¤é¢
    if 'amount' in daily_df.columns:
        daily_df['amount'] = pd.to_numeric(daily_df['amount'], errors='coerce')
        avg_amount_20d = daily_df['amount'].tail(20).mean()
        if avg_amount_20d < params['min_avg_amount_20d']:
            return False, f"æˆäº¤é¢ä¸è¶³({avg_amount_20d/1e8:.2f}äº¿)"
    
    return True, ""


def judge_monthly_trend(monthly_df: pd.DataFrame, config: dict) -> str:
    """
    åˆ¤å®šæœˆçº¿å¤§åŠ¿
    
    Returns:
        "UPTREND" | "BASE_BUILDING" | "DOWNTREND"
    """
    if monthly_df is None or len(monthly_df) < 12:
        return "DOWNTREND"
    
    # è®¡ç®—æŒ‡æ ‡
    monthly_df = calculate_all_indicators(monthly_df, freq='monthly')
    
    params_up = config['monthly']['uptrend']
    params_base = config['monthly']['base_building']
    
    latest = monthly_df.iloc[-1]
    
    # ===== ä¸Šå‡è¶‹åŠ¿åˆ¤å®š =====
    uptrend_score = 0
    uptrend_checks = 0
    
    # 1. æ”¶ç›˜ä»·é«˜äº10æœˆå‡çº¿
    if params_up['close_above_ma10'] and 'MA10' in monthly_df.columns:
        if not pd.isna(latest['MA10']) and latest['close'] > latest['MA10']:
            uptrend_score += 1
        uptrend_checks += 1
    
    # 2. MA10æ–œç‡å‘ä¸Š
    if 'MA10_slope' in monthly_df.columns:
        if not pd.isna(latest['MA10_slope']) and latest['MA10_slope'] >= params_up['ma10_slope_min']:
            uptrend_score += 1
        uptrend_checks += 1
    
    # 3. å‡çº¿å¤šå¤´æ’åˆ—
    if params_up['ma_alignment_check']:
        ma_aligned = check_ma_alignment(monthly_df.tail(1), ['MA5', 'MA10', 'MA20'], ascending=True)
        if ma_aligned.iloc[0]:
            uptrend_score += 1
        uptrend_checks += 1
    
    # 4. 6ä¸ªæœˆæ¶¨å¹…
    if len(monthly_df) >= 6:
        gain_6m = (latest['close'] - monthly_df.iloc[-7]['close']) / monthly_df.iloc[-7]['close']
        if gain_6m >= params_up['gain_6m_min']:
            uptrend_score += 1
        uptrend_checks += 1
    
    # å¦‚æœä¸Šå‡è¶‹åŠ¿å¾—åˆ†>=75%ï¼Œåˆ¤å®šä¸ºä¸Šå‡è¶‹åŠ¿
    if uptrend_checks > 0 and uptrend_score / uptrend_checks >= 0.75:
        return "UPTREND"
    
    # ===== åº•éƒ¨ç­‘åº•åˆ¤å®š =====
    if len(monthly_df) >= 24:
        base_score = 0
        base_checks = 0
        
        # 1. 24ä¸ªæœˆå†…æœ‰è¿‡æ·±åº¦å›è°ƒ
        recent_24m = monthly_df.tail(24)
        max_price = recent_24m['close'].max()
        min_price = recent_24m['close'].min()
        drawdown = (max_price - min_price) / max_price
        
        if drawdown >= params_base['max_drawdown_24m']:
            base_score += 1
        base_checks += 1
        
        # 2. æ¨ªç›˜4-8ä¸ªæœˆä¸”æ³¢åŠ¨æ”¶æ•›
        for lookback in range(params_base['consolidation_months_min'], 
                             params_base['consolidation_months_max'] + 1):
            if len(monthly_df) >= lookback:
                consolidation_period = monthly_df.tail(lookback)
                volatility = consolidation_period['close'].std() / consolidation_period['close'].mean()
                
                if volatility < params_base['volatility_threshold']:
                    base_score += 1
                    break
        base_checks += 1
        
        # 3. MA10æ–œç‡æ¥è¿‘0ï¼ˆèµ°å¹³ï¼‰
        if 'MA10_slope' in monthly_df.columns and not pd.isna(latest['MA10_slope']):
            if params_base['ma10_slope_min'] <= latest['MA10_slope'] <= params_base['ma10_slope_max']:
                base_score += 1
            base_checks += 1
        
        # 4. é‡èƒ½é…åˆï¼ˆä¸Šæ¶¨æœˆé‡èƒ½>ä¸‹è·Œæœˆé‡èƒ½ï¼‰
        if params_base['volume_cooperation']:
            volume_analysis = analyze_volume_pattern(monthly_df.tail(6), lookback=6)
            if volume_analysis['volume_cooperation']:
                base_score += 1
            base_checks += 1
        
        # å¦‚æœç­‘åº•å¾—åˆ†>=75%ï¼Œåˆ¤å®šä¸ºåº•éƒ¨ç­‘åº•
        if base_checks > 0 and base_score / base_checks >= 0.75:
            return "BASE_BUILDING"
    
    # å…¶ä»–æƒ…å†µåˆ¤å®šä¸ºä¸‹è·Œè¶‹åŠ¿
    return "DOWNTREND"


def check_weekly_structure(weekly_df: pd.DataFrame, config: dict) -> dict:
    """
    æ£€æŸ¥å‘¨çº¿ç»“æ„
    
    Returns:
        {
            "passed": bool,
            "score": int (0-100),
            "details": dict
        }
    """
    if weekly_df is None or len(weekly_df) < 40:
        return {"passed": False, "score": 0, "details": {}}
    
    # è®¡ç®—æŒ‡æ ‡
    weekly_df = calculate_all_indicators(weekly_df, freq='weekly')
    
    params = config['weekly']['mandatory']
    latest = weekly_df.iloc[-1]
    
    result = {
        "passed": True,
        "score": 0,
        "details": {}
    }
    
    # ===== å¿…è¦æ¡ä»¶æ£€æŸ¥ =====
    mandatory_score = 0
    mandatory_total = 0
    
    # 1. æ”¶ç›˜ä»·é«˜äº40å‘¨å‡çº¿
    if params['close_above_ma40'] and 'MA40' in weekly_df.columns:
        if pd.isna(latest['MA40']) or latest['close'] <= latest['MA40']:
            result['passed'] = False
            result['details']['ma40_check'] = False
        else:
            mandatory_score += 20
            result['details']['ma40_check'] = True
        mandatory_total += 20
    
    # 2. MA40æ–œç‡>=0
    if 'MA40_slope' in weekly_df.columns:
        if pd.isna(latest['MA40_slope']) or latest['MA40_slope'] < params['ma40_slope_min']:
            result['passed'] = False
            result['details']['ma40_slope_check'] = False
        else:
            mandatory_score += 15
            result['details']['ma40_slope_check'] = True
        mandatory_total += 15
    
    # 3. æ”¶ç›˜ä»·é«˜äº10å‘¨å‡çº¿
    if params['close_above_ma10'] and 'MA10' in weekly_df.columns:
        if pd.isna(latest['MA10']) or latest['close'] <= latest['MA10']:
            result['passed'] = False
            result['details']['ma10_check'] = False
        else:
            mandatory_score += 15
            result['details']['ma10_check'] = True
        mandatory_total += 15
    
    # 4. MACDæ­£å€¼æˆ–é‡‘å‰
    if params['macd_positive_or_golden']:
        macd_ok = False
        if 'MACD_hist' in weekly_df.columns and not pd.isna(latest['MACD_hist']):
            if latest['MACD_hist'] > 0:
                macd_ok = True
            elif detect_macd_golden_cross(weekly_df, lookback=params['macd_lookback']):
                macd_ok = True
        
        if not macd_ok:
            result['passed'] = False
            result['details']['macd_check'] = False
        else:
            mandatory_score += 20
            result['details']['macd_check'] = True
        mandatory_total += 20
    
    # 5. é‡èƒ½é…åˆï¼ˆä¸Šæ¶¨å‘¨>ä¸‹è·Œå‘¨ï¼‰
    volume_analysis = analyze_volume_pattern(
        weekly_df.tail(params['volume_lookback']), 
        lookback=params['volume_lookback']
    )
    
    if volume_analysis['volume_ratio'] < params['volume_ratio_min']:
        result['passed'] = False
        result['details']['volume_check'] = False
    else:
        mandatory_score += 30
        result['details']['volume_check'] = True
    mandatory_total += 30
    
    result['details']['volume_ratio'] = volume_analysis['volume_ratio']
    
    # å¦‚æœä¸é€šè¿‡å¿…è¦æ¡ä»¶ï¼Œç›´æ¥è¿”å›
    if not result['passed']:
        result['score'] = 0
        return result
    
    # é€šè¿‡äº†å¿…è¦æ¡ä»¶ï¼ŒåŸºç¡€åˆ†å°±æ˜¯mandatory_score
    result['score'] = mandatory_score
    
    # ===== é¢å¤–åŠ åˆ†é¡¹ï¼ˆå¢åŠ åŒºåˆ†åº¦ï¼‰ =====
    bonus_score = 0
    
    # 1. MA40æ–œç‡åŠ åˆ†ï¼ˆæœ€å¤š10åˆ†ï¼‰
    if 'MA40_slope' in weekly_df.columns and not pd.isna(latest['MA40_slope']):
        if latest['MA40_slope'] > 0.02:
            bonus_score += 10  # å¼ºåŠ²ä¸Šå‡
        elif latest['MA40_slope'] > 0.01:
            bonus_score += 5   # æ¸©å’Œä¸Šå‡
    
    # 2. MACDå¼ºåº¦åŠ åˆ†ï¼ˆæœ€å¤š10åˆ†ï¼‰
    if 'MACD_hist' in weekly_df.columns and not pd.isna(latest['MACD_hist']):
        if latest['MACD_hist'] > latest['close'] * 0.02:
            bonus_score += 10  # MACDæŸ±çŠ¶å¾ˆå¼º
        elif latest['MACD_hist'] > latest['close'] * 0.01:
            bonus_score += 5
    
    # 3. é‡èƒ½å¼ºåº¦åŠ åˆ†ï¼ˆæœ€å¤š10åˆ†ï¼‰
    if volume_analysis['volume_ratio'] > 1.5:
        bonus_score += 10  # ä¸Šæ¶¨å‘¨é‡èƒ½å¾ˆå¼º
    elif volume_analysis['volume_ratio'] > 1.3:
        bonus_score += 5
    
    # 4. ä»·æ ¼ä½ç½®åŠ åˆ†ï¼ˆæœ€å¤š10åˆ†ï¼‰
    if 'MA10' in weekly_df.columns and not pd.isna(latest['MA10']):
        price_above_ma10 = (latest['close'] - latest['MA10']) / latest['MA10']
        if 0.02 < price_above_ma10 < 0.10:  # åˆšç«™ç¨³MA10ï¼Œæœªæ¶¨å¤ªå¤š
            bonus_score += 10
        elif 0.10 <= price_above_ma10 < 0.20:
            bonus_score += 5
    
    # 5. RSIé€‚ä¸­åŠ åˆ†ï¼ˆæœ€å¤š5åˆ†ï¼‰
    if 'RSI' in weekly_df.columns and not pd.isna(latest['RSI']):
        if 50 < latest['RSI'] < 70:  # RSIåœ¨å¥åº·åŒºé—´
            bonus_score += 5
    
    # 6. è¿ç»­æ”¾é‡åŠ åˆ†ï¼ˆæœ€å¤š15åˆ†ï¼‰
    consecutive_vol_config = config['weekly']['mandatory'].get('consecutive_volume', {})
    if consecutive_vol_config.get('enabled', False) and len(weekly_df) >= 8:
        min_weeks = consecutive_vol_config.get('min_weeks', 2)
        vol_increase_ratio = consecutive_vol_config.get('volume_increase_ratio', 1.2)
        max_bonus = consecutive_vol_config.get('max_bonus', 15)
        
        # è®¡ç®—è¿ç»­æ”¾é‡å‘¨æ•°
        recent_weeks = weekly_df.tail(8)
        avg_volume_4w = recent_weeks.head(4)['volume'].mean()  # å‰4å‘¨å‡é‡ä½œä¸ºåŸºå‡†
        
        consecutive_count = 0
        for i in range(4, len(recent_weeks)):  # æ£€æŸ¥æœ€è¿‘4å‘¨
            week_volume = recent_weeks.iloc[i]['volume']
            if week_volume > avg_volume_4w * vol_increase_ratio:
                consecutive_count += 1
            else:
                consecutive_count = 0  # ä¸­æ–­åˆ™é‡ç½®
        
        # æ ¹æ®è¿ç»­æ”¾é‡å‘¨æ•°åŠ åˆ†
        if consecutive_count >= min_weeks:
            # è¿ç»­2å‘¨+5åˆ†ï¼Œ3å‘¨+10åˆ†ï¼Œ4å‘¨+15åˆ†
            vol_bonus = min((consecutive_count - min_weeks + 1) * 5, max_bonus)
            bonus_score += vol_bonus
            result['details']['consecutive_volume_weeks'] = consecutive_count
    
    # æ›´æ–°æ€»åˆ†ï¼ˆåŸºç¡€åˆ†+åŠ åˆ†ï¼Œæœ€å¤š160åˆ†ï¼Œåé¢ä¼šå½’ä¸€åŒ–ï¼‰
    result['score'] = mandatory_score + bonus_score
    
    # å½’ä¸€åŒ–åˆ°0-100ï¼ˆå¯é€‰ï¼Œä¿æŒå…¼å®¹ï¼‰
    # result['score'] = min(result['score'], 100)
    
    return result


def process_single_stock(stock_code: str, data_dir: Path, config: dict) -> Optional[dict]:
    """
    å¤„ç†å•åªè‚¡ç¥¨
    
    Returns:
        è‚¡ç¥¨åˆ†æç»“æœå­—å…¸æˆ–None
    """
    # 1. åŠ è½½æ•°æ®
    daily_df = load_kline_data(stock_code, data_dir, 'daily')
    weekly_df = load_kline_data(stock_code, data_dir, 'weekly')
    monthly_df = load_kline_data(stock_code, data_dir, 'monthly')
    
    if daily_df is None or weekly_df is None or monthly_df is None:
        return None
    
    # 2. é¢„è¿‡æ»¤
    passed, reason = pre_filter_stock(stock_code, daily_df, config)
    if not passed:
        return None
    
    # 3. æœˆçº¿å¤§åŠ¿åˆ¤å®š
    monthly_trend = judge_monthly_trend(monthly_df, config)
    if monthly_trend == "DOWNTREND":
        return None
    
    # 4. å‘¨çº¿ç»“æ„éªŒè¯
    weekly_result = check_weekly_structure(weekly_df, config)
    if not weekly_result['passed']:
        return None
    
    # 5. è·å–æœ€æ–°ä»·æ ¼ä¿¡æ¯
    latest_daily = daily_df.iloc[-1]
    latest_weekly = weekly_df.iloc[-1]
    
    # 6. æ„å»ºç»“æœ
    result = {
        'code': stock_code,
        'name': latest_daily.get('code', stock_code),
        'monthly_trend': monthly_trend,
        'weekly_score': weekly_result['score'],
        'current_price': float(latest_daily['close']),
        'ma40_weekly': float(latest_weekly.get('MA40', 0)) if 'MA40' in latest_weekly else 0,
        'volume_ratio': weekly_result['details'].get('volume_ratio', 0),
        'date': latest_daily['date'],
        'details': weekly_result['details']
    }
    
    return result


def main() -> int:
    args = parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # ç¡®å®šæ•°æ®ç›®å½•å’Œè¾“å‡ºè·¯å¾„
    data_dir = Path(args.data_dir if args.data_dir else config['paths']['kline_data_dir'])
    output_dir = Path(config['paths']['output_dir'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = args.output if args.output else output_dir / config['paths']['watchlist_file']
    
    print("=" * 80)
    print("å‘¨çº¿çº§åˆ«è‚¡ç¥¨ç­›é€‰å™¨")
    print("=" * 80)
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print("=" * 80)
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
    if not data_dir.exists():
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return 1
    
    stock_dirs = [d for d in data_dir.iterdir() if d.is_dir()]
    print(f"å…±æ‰¾åˆ° {len(stock_dirs)} åªè‚¡ç¥¨æ•°æ®")
    
    # å¤„ç†æ¯åªè‚¡ç¥¨
    results = []
    print("\nå¼€å§‹ç­›é€‰...")
    
    for stock_dir in tqdm(stock_dirs, desc="ç­›é€‰è¿›åº¦"):
        stock_code = stock_dir.name
        
        try:
            result = process_single_stock(stock_code, data_dir, config)
            if result:
                results.append(result)
        except Exception as e:
            print(f"\nå¤„ç† {stock_code} æ—¶å‡ºé”™: {e}")
            continue
    
    # ç”ŸæˆæŠ¥å‘Š
    if not results:
        print("\næœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨ï¼")
        return 0
    
    # è½¬æ¢ä¸ºDataFrame
    results_df = pd.DataFrame(results)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    prefer_base = config.get('output', {}).get('watchlist', {}).get('prefer_base_building', False)
    
    if prefer_base:
        # ä¼˜å…ˆåº•éƒ¨ç­‘åº•ï¼šBASE_BUILDINGæ’å‰é¢ï¼ŒåŒç±»å‹æŒ‰è¯„åˆ†æ’åº
        results_df['sort_priority'] = results_df['monthly_trend'].map({
            'BASE_BUILDING': 1,  # ç­‘åº•è‚¡ç¥¨ä¼˜å…ˆçº§æœ€é«˜
            'UPTREND': 2         # ä¸Šå‡è¶‹åŠ¿å…¶æ¬¡
        })
        results_df = results_df.sort_values(['sort_priority', 'weekly_score'], ascending=[True, False])
    else:
        # é»˜è®¤ï¼šåªæŒ‰å‘¨çº¿è¯„åˆ†æ’åº
        results_df = results_df.sort_values('weekly_score', ascending=False)
    
    # é™åˆ¶æ•°é‡
    max_stocks = config['output']['watchlist']['max_stocks']
    results_df = results_df.head(max_stocks)
    
    # ä¿å­˜ç»“æœ
    output_cols = ['code', 'name', 'monthly_trend', 'weekly_score', 
                   'current_price', 'ma40_weekly', 'volume_ratio', 'date']
    results_df[output_cols].to_csv(output_path, index=False, encoding='utf-8-sig')
    
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ç­›é€‰å®Œæˆï¼")
    print("=" * 80)
    print(f"ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨æ•°é‡: {len(results_df)}")
    print(f"\næœˆçº¿çŠ¶æ€åˆ†å¸ƒ:")
    print(results_df['monthly_trend'].value_counts())
    print(f"\nå‘¨çº¿è¯„åˆ†ç»Ÿè®¡:")
    print(f"  å¹³å‡åˆ†: {results_df['weekly_score'].mean():.2f}")
    print(f"  æœ€é«˜åˆ†: {results_df['weekly_score'].max():.0f}")
    print(f"  æœ€ä½åˆ†: {results_df['weekly_score'].min():.0f}")
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("=" * 80)
    
    # æ˜¾ç¤ºå‰10åªè‚¡ç¥¨
    print("\nğŸ”¥ è¯„åˆ†æœ€é«˜çš„å‰10åªè‚¡ç¥¨:")
    print(results_df[output_cols].head(10).to_string(index=False))
    
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

