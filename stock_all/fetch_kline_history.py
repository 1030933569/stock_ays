"""
è·å–Aè‚¡æ‰€æœ‰å…¬å¸çš„Kçº¿å†å²æ•°æ®
- æœˆçº¿ï¼š10å¹´
- å‘¨çº¿ï¼š5å¹´
- æ—¥çº¿ï¼š1å¹´

ä½¿ç”¨ baostock åº“è·å–æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°CSVæ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
    python stock_all/fetch_kline_history.py --output-dir ./kline_data
"""

from __future__ import annotations

import argparse
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional

import baostock as bs
import pandas as pd
from tqdm import tqdm


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="è·å–Aè‚¡æ‰€æœ‰å…¬å¸çš„Kçº¿å†å²æ•°æ®ï¼ˆæœˆçº¿10å¹´ã€å‘¨çº¿5å¹´ã€æ—¥çº¿1å¹´ï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        default="./kline_data",
        help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: ./kline_dataï¼‰",
    )
    parser.add_argument(
        "--stock-type",
        default="1",
        help="è‚¡ç¥¨ç±»å‹ï¼š1-è‚¡ç¥¨ï¼Œ2-æŒ‡æ•°ï¼ˆé»˜è®¤: 1ï¼‰",
    )
    parser.add_argument(
        "--encoding",
        default="utf-8-sig",
        help="CSVæ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤: utf-8-sigï¼ŒExcelå¯ç›´æ¥æ‰“å¼€ï¼‰",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.1,
        help="æ¯æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 0.1ï¼‰",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶è·å–çš„è‚¡ç¥¨æ•°é‡ï¼Œç”¨äºæµ‹è¯•ï¼ˆé»˜è®¤: Noneï¼Œè·å–æ‰€æœ‰ï¼‰",
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        default=True,
        help="è·³è¿‡å·²ä¸‹è½½çš„è‚¡ç¥¨ï¼Œæ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤: Trueï¼‰",
    )
    parser.add_argument(
        "--incremental",
        action="store_true",
        default=True,
        help="å¢é‡æ›´æ–°æ¨¡å¼ï¼šåªä¸‹è½½æ–°å¢æ•°æ®å¹¶è¿½åŠ ï¼ˆé»˜è®¤: Trueï¼‰",
    )
    return parser.parse_args()


def login() -> None:
    """ç™»å½• baostock"""
    lg = bs.login()
    if lg.error_code != "0":
        raise RuntimeError(f"Baostock ç™»å½•å¤±è´¥: {lg.error_msg}")
    print("Baostock ç™»å½•æˆåŠŸ", flush=True)


def logout() -> None:
    """ç™»å‡º baostock"""
    bs.logout()
    print("Baostock ç™»å‡ºæˆåŠŸ", flush=True)


def get_stock_list(stock_type: str = "1") -> pd.DataFrame:
    """
    è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨
    
    Args:
        stock_type: è‚¡ç¥¨ç±»å‹ï¼Œ1-è‚¡ç¥¨ï¼Œ2-æŒ‡æ•°
        
    Returns:
        åŒ…å«è‚¡ç¥¨ä»£ç å’Œåç§°çš„DataFrame
    """
    print("æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...", flush=True)
    # ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„æ—¥æœŸï¼ˆæœ€è¿‘çš„äº¤æ˜“æ—¥ï¼‰é¿å…å‘¨æœ«æˆ–èŠ‚å‡æ—¥æ²¡æœ‰æ•°æ®
    # å¯ä»¥ä½¿ç”¨ 2024-11-01 è¿™æ ·çš„æ—¥æœŸ
    query_date = "2024-11-01"  # ä½¿ç”¨ä¸€ä¸ªç¡®å®šæœ‰æ•°æ®çš„äº¤æ˜“æ—¥
    rs = bs.query_all_stock(day=query_date)
    
    if rs.error_code != "0":
        raise RuntimeError(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {rs.error_msg}")
    
    data_list = []
    while rs.next():
        row = rs.get_row_data()
        data_list.append(row)
    
    df = pd.DataFrame(data_list, columns=rs.fields)
    
    # ç­›é€‰è‚¡ç¥¨ç±»å‹
    if stock_type == "1":
        # åªä¿ç•™è‚¡ç¥¨ï¼ˆsh.6xxxxx æˆ– sz.0xxxxx, sz.3xxxxxï¼‰
        # ä½¿ç”¨ str.containsï¼Œ(?:...) è¡¨ç¤ºéæ•è·ç»„é¿å…è­¦å‘Š
        df = df[df["code"].str.contains(r"^(?:sh\.6|sz\.0|sz\.3)", regex=True, na=False)]
    
    print(f"å…±è·å–åˆ° {len(df)} åªè‚¡ç¥¨", flush=True)
    return df


def calculate_date_range(years: int) -> tuple[str, str]:
    """
    è®¡ç®—æ—¥æœŸèŒƒå›´
    
    Args:
        years: å¹´æ•°
        
    Returns:
        (start_date, end_date) æ ¼å¼: YYYY-MM-DD
    """
    end_date = datetime.today()
    start_date = end_date - timedelta(days=years * 365)
    return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")


def get_last_date_from_csv(csv_path: Path) -> Optional[str]:
    """
    ä»CSVæ–‡ä»¶ä¸­è·å–æœ€åä¸€æ¡æ•°æ®çš„æ—¥æœŸ
    
    Args:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        æœ€åæ—¥æœŸï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        if not csv_path.exists():
            return None
        df = pd.read_csv(csv_path)
        if len(df) == 0:
            return None
        # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸåˆ—
        last_date = df.iloc[-1, 0]
        return str(last_date)
    except:
        return None


def fetch_kline_data(
    code: str,
    start_date: str,
    end_date: str,
    frequency: str = "d",
    adjustflag: str = "3",
    max_retries: int = 3
) -> Optional[pd.DataFrame]:
    """
    è·å–å•åªè‚¡ç¥¨çš„Kçº¿æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚ sh.600000
        start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
        end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD
        frequency: æ•°æ®é¢‘ç‡ï¼Œd-æ—¥ï¼Œw-å‘¨ï¼Œm-æœˆ
        adjustflag: å¤æƒç±»å‹ï¼Œ1-åå¤æƒï¼Œ2-å‰å¤æƒï¼Œ3-ä¸å¤æƒ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        Kçº¿æ•°æ®DataFrameï¼Œå¤±è´¥è¿”å›None
    """
    # æ ¹æ®é¢‘ç‡é€‰æ‹©ä¸åŒçš„å­—æ®µ
    if frequency == "d":
        fields = "date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST"
    else:  # å‘¨çº¿å’Œæœˆçº¿
        fields = "date,code,open,high,low,close,volume,amount,adjustflag,turn,pctChg"
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            rs = bs.query_history_k_data_plus(
                code,
                fields,
                start_date=start_date,
                end_date=end_date,
                frequency=frequency,
                adjustflag=adjustflag
            )
            
            if rs.error_code != "0":
                if attempt < max_retries - 1:
                    time.sleep(0.5)  # ç­‰å¾…åé‡è¯•
                    continue
                else:
                    return None
            
            data_list = []
            while (rs.error_code == '0') & rs.next():
                row = rs.get_row_data()
                data_list.append(row)
            
            if not data_list:
                return None
            
            df = pd.DataFrame(data_list, columns=rs.fields)
            return df
            
        except Exception as e:
            # æ•è·ç¼–ç é”™è¯¯ã€ç½‘ç»œé”™è¯¯ç­‰å¼‚å¸¸
            if attempt < max_retries - 1:
                time.sleep(1)  # ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
                continue
            else:
                # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥ï¼Œè¿”å›None
                return None
    
    return None


def save_kline_data(
    df: pd.DataFrame,
    output_path: Path,
    encoding: str = "utf-8-sig",
    append: bool = False
) -> None:
    """
    ä¿å­˜Kçº¿æ•°æ®åˆ°CSVæ–‡ä»¶
    
    Args:
        df: æ•°æ®DataFrame
        output_path: è¾“å‡ºè·¯å¾„
        encoding: ç¼–ç æ ¼å¼
        append: æ˜¯å¦è¿½åŠ æ¨¡å¼ï¼ˆTrue=è¿½åŠ æ–°æ•°æ®ï¼ŒFalse=è¦†ç›–æ•´ä¸ªæ–‡ä»¶ï¼‰
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    if append and output_path.exists():
        # è¿½åŠ æ¨¡å¼ï¼šè¯»å–åŸæ•°æ®ï¼Œåˆå¹¶åå»é‡
        try:
            existing_df = pd.read_csv(output_path, encoding=encoding)
            # åˆå¹¶æ•°æ®
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            # æŒ‰æ—¥æœŸå»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
            date_col = combined_df.columns[0]  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯æ—¥æœŸ
            combined_df = combined_df.drop_duplicates(subset=[date_col], keep='last')
            # æŒ‰æ—¥æœŸæ’åº
            combined_df = combined_df.sort_values(by=date_col)
            combined_df.to_csv(output_path, index=False, encoding=encoding)
        except:
            # å¦‚æœè¿½åŠ å¤±è´¥ï¼Œåˆ™è¦†ç›–
            df.to_csv(output_path, index=False, encoding=encoding)
    else:
        # è¦†ç›–æ¨¡å¼
        df.to_csv(output_path, index=False, encoding=encoding)


def is_stock_downloaded(code: str, output_dir: Path, check_freshness: bool = True) -> bool:
    """
    æ£€æŸ¥è‚¡ç¥¨æ•°æ®æ˜¯å¦å·²å®Œæ•´ä¸‹è½½ä¸”æ•°æ®æ–°é²œ
    
    Args:
        code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ sh.600000ï¼‰
        output_dir: è¾“å‡ºç›®å½•
        check_freshness: æ˜¯å¦æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼ˆé»˜è®¤Trueï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨7å¤©å†…æ›´æ–°è¿‡ï¼‰
        
    Returns:
        Trueè¡¨ç¤ºå·²ä¸‹è½½ä¸”æ•°æ®æ–°é²œï¼ŒFalseè¡¨ç¤ºéœ€è¦ï¼ˆé‡æ–°ï¼‰ä¸‹è½½
    """
    # æ¸…ç†è‚¡ç¥¨ä»£ç 
    clean_code = code.replace("sh.", "").replace("sz.", "")
    stock_dir = output_dir / clean_code
    
    # æ£€æŸ¥3ä¸ªæ–‡ä»¶æ˜¯å¦éƒ½å­˜åœ¨ä¸”ä¸ä¸ºç©º
    required_files = [
        stock_dir / f"{clean_code}_daily_1y.csv",
        stock_dir / f"{clean_code}_weekly_5y.csv",
        stock_dir / f"{clean_code}_monthly_10y.csv"
    ]
    
    for file_path in required_files:
        if not file_path.exists():
            return False
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºï¼ˆè‡³å°‘æœ‰è¡¨å¤´ï¼‰
        try:
            df = pd.read_csv(file_path)
            if len(df) == 0:
                return False
        except:
            return False
        
        # æ£€æŸ¥æ•°æ®æ–°é²œåº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if check_freshness:
            try:
                # æ£€æŸ¥æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´
                file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                days_old = (datetime.today() - file_mtime).days
                
                # å¦‚æœæ–‡ä»¶è¶…è¿‡7å¤©æœªæ›´æ–°ï¼Œè®¤ä¸ºæ•°æ®è¿‡æœŸ
                if days_old > 7:
                    return False
            except:
                return False
    
    return True


def process_stock(
    code: str,
    stock_name: str,
    output_dir: Path,
    encoding: str,
    delay: float,
    incremental: bool = True
) -> dict:
    """
    å¤„ç†å•åªè‚¡ç¥¨ï¼Œè·å–å…¶Kçº¿æ•°æ®ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
        stock_name: è‚¡ç¥¨åç§°
        output_dir: è¾“å‡ºç›®å½•
        encoding: æ–‡ä»¶ç¼–ç 
        delay: è¯·æ±‚å»¶è¿Ÿ
        incremental: æ˜¯å¦å¢é‡æ›´æ–°ï¼ˆTrue=åªä¸‹è½½æ–°æ•°æ®å¹¶è¿½åŠ ï¼ŒFalse=é‡æ–°ä¸‹è½½å…¨éƒ¨ï¼‰
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {"code": code, "name": stock_name, "daily": 0, "weekly": 0, "monthly": 0, "success": True}
    
    try:
        # æ¸…ç†è‚¡ç¥¨ä»£ç ï¼Œç”¨äºæ–‡ä»¶åï¼ˆå»æ‰ sh. æˆ– sz. å‰ç¼€ï¼‰
        clean_code = code.replace("sh.", "").replace("sz.", "")
        
        # åˆ›å»ºè‚¡ç¥¨ä¸“å±ç›®å½•
        stock_dir = output_dir / clean_code
        
        # 1. è·å–æ—¥çº¿æ•°æ®ï¼ˆ1å¹´ï¼‰
        daily_file = stock_dir / f"{clean_code}_daily_1y.csv"
        if incremental and daily_file.exists():
            # å¢é‡æ¨¡å¼ï¼šä»æœ€åæ—¥æœŸå¼€å§‹ä¸‹è½½
            last_date = get_last_date_from_csv(daily_file)
            if last_date:
                # ä»æœ€åæ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹
                start_date = (datetime.strptime(last_date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")
            else:
                start_date, _ = calculate_date_range(1)
            _, end_date = calculate_date_range(0)  # åˆ°ä»Šå¤©
            daily_df = fetch_kline_data(code, start_date, end_date, frequency="d", max_retries=3)
            if daily_df is not None and not daily_df.empty:
                save_kline_data(daily_df, daily_file, encoding, append=True)
                stats["daily"] = len(daily_df)
        else:
            # å®Œæ•´æ¨¡å¼ï¼šä¸‹è½½å…¨éƒ¨æ•°æ®
            start_date, end_date = calculate_date_range(1)
            daily_df = fetch_kline_data(code, start_date, end_date, frequency="d", max_retries=3)
            if daily_df is not None and not daily_df.empty:
                save_kline_data(daily_df, daily_file, encoding, append=False)
                stats["daily"] = len(daily_df)
        time.sleep(delay)
        
        # 2. è·å–å‘¨çº¿æ•°æ®ï¼ˆ5å¹´ï¼‰
        weekly_file = stock_dir / f"{clean_code}_weekly_5y.csv"
        if incremental and weekly_file.exists():
            last_date = get_last_date_from_csv(weekly_file)
            if last_date:
                start_date = (datetime.strptime(last_date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")
            else:
                start_date, _ = calculate_date_range(5)
            _, end_date = calculate_date_range(0)
            weekly_df = fetch_kline_data(code, start_date, end_date, frequency="w", max_retries=3)
            if weekly_df is not None and not weekly_df.empty:
                save_kline_data(weekly_df, weekly_file, encoding, append=True)
                stats["weekly"] = len(weekly_df)
        else:
            start_date, end_date = calculate_date_range(5)
            weekly_df = fetch_kline_data(code, start_date, end_date, frequency="w", max_retries=3)
            if weekly_df is not None and not weekly_df.empty:
                save_kline_data(weekly_df, weekly_file, encoding, append=False)
                stats["weekly"] = len(weekly_df)
        time.sleep(delay)
        
        # 3. è·å–æœˆçº¿æ•°æ®ï¼ˆ10å¹´ï¼‰
        monthly_file = stock_dir / f"{clean_code}_monthly_10y.csv"
        if incremental and monthly_file.exists():
            last_date = get_last_date_from_csv(monthly_file)
            if last_date:
                start_date = (datetime.strptime(last_date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")
            else:
                start_date, _ = calculate_date_range(10)
            _, end_date = calculate_date_range(0)
            monthly_df = fetch_kline_data(code, start_date, end_date, frequency="m", max_retries=3)
            if monthly_df is not None and not monthly_df.empty:
                save_kline_data(monthly_df, monthly_file, encoding, append=True)
                stats["monthly"] = len(monthly_df)
        else:
            start_date, end_date = calculate_date_range(10)
            monthly_df = fetch_kline_data(code, start_date, end_date, frequency="m", max_retries=3)
            if monthly_df is not None and not monthly_df.empty:
                save_kline_data(monthly_df, monthly_file, encoding, append=False)
                stats["monthly"] = len(monthly_df)
        time.sleep(delay)
        
    except Exception as e:
        # æ•è·ä»»ä½•å¼‚å¸¸ï¼Œæ ‡è®°ä¸ºå¤±è´¥ä½†ä¸ä¸­æ–­æ•´ä½“æµç¨‹
        stats["success"] = False
    
    return stats


def main() -> int:
    args = parse_args()
    output_dir = Path(args.output_dir).expanduser().resolve()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60, flush=True)
    print("Aè‚¡Kçº¿å†å²æ•°æ®è·å–å·¥å…·ï¼ˆå¢é‡æ›´æ–°ç‰ˆï¼‰", flush=True)
    print("=" * 60, flush=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}", flush=True)
    print(f"æ•°æ®èŒƒå›´: æœˆçº¿10å¹´ã€å‘¨çº¿5å¹´ã€æ—¥çº¿1å¹´", flush=True)
    print(f"è¯·æ±‚å»¶è¿Ÿ: {args.delay}ç§’", flush=True)
    print(f"æ›´æ–°æ¨¡å¼: {'å¢é‡è¿½åŠ ' if args.incremental else 'å®Œæ•´ä¸‹è½½'}", flush=True)
    if args.incremental:
        print("ğŸ’¡ å°†è‡ªåŠ¨è¿½åŠ æœ€æ–°æ•°æ®ï¼Œå¿«é€Ÿé«˜æ•ˆï¼", flush=True)
    print("=" * 60, flush=True)
    
    try:
        # ç™»å½•
        login()
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = get_stock_list(args.stock_type)
        
        # å¦‚æœè®¾ç½®äº†limitï¼Œåªå¤„ç†å‰Nåªè‚¡ç¥¨
        if args.limit:
            stock_list = stock_list.head(args.limit)
            print(f"æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {args.limit} åªè‚¡ç¥¨", flush=True)
        
        # ç­›é€‰éœ€è¦ä¸‹è½½çš„è‚¡ç¥¨
        stocks_to_download = []
        skipped_count = 0
        
        if args.incremental:
            # å¢é‡æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰è‚¡ç¥¨ï¼ˆå¿«é€Ÿè¿½åŠ æ–°æ•°æ®ï¼‰
            print("\nå¢é‡æ›´æ–°æ¨¡å¼ï¼šå°†ä¸ºæ‰€æœ‰è‚¡ç¥¨è¿½åŠ æœ€æ–°æ•°æ®", flush=True)
            stocks_to_download = [row for idx, row in stock_list.iterrows()]
        elif args.skip_existing:
            # è·³è¿‡æ¨¡å¼ï¼šåªå¤„ç†ç¼ºå¤±çš„è‚¡ç¥¨
            print("\næ£€æŸ¥å·²ä¸‹è½½çš„è‚¡ç¥¨...", flush=True)
            for idx, row in stock_list.iterrows():
                code = row["code"]
                if is_stock_downloaded(code, output_dir, check_freshness=False):
                    skipped_count += 1
                else:
                    stocks_to_download.append(row)
            
            print(f"å·²ä¸‹è½½: {skipped_count} åª", flush=True)
            print(f"éœ€è¦ä¸‹è½½: {len(stocks_to_download)} åª", flush=True)
        else:
            stocks_to_download = [row for idx, row in stock_list.iterrows()]
        
        if not stocks_to_download and not args.incremental:
            print("\næ‰€æœ‰è‚¡ç¥¨æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€ä¸‹è½½ï¼", flush=True)
            return 0
        
        # ä¸²è¡Œå¤„ç†ï¼ˆbaostockä¸æ”¯æŒå¹¶å‘ï¼‰
        all_stats = []
        failed_stocks = []
        mode_desc = "å¢é‡è¿½åŠ æ¨¡å¼" if args.incremental else "å®Œæ•´ä¸‹è½½æ¨¡å¼"
        print(f"\nå¼€å§‹æ›´æ–°Kçº¿æ•°æ®ï¼ˆ{mode_desc}ï¼‰...", flush=True)
        
        for row in tqdm(stocks_to_download, desc="ä¸‹è½½è¿›åº¦"):
            code = row["code"]
            stock_name = row.get("code_name", "")
            
            try:
                stats = process_stock(code, stock_name, output_dir, args.encoding, args.delay, args.incremental)
                all_stats.append(stats)
                
                # è®°å½•å¤±è´¥çš„è‚¡ç¥¨
                if not stats.get("success", True):
                    failed_stocks.append(f"{code}({stock_name})")
            except Exception as e:
                failed_stocks.append(f"{code}({stock_name}): {str(e)[:50]}")
                continue
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_df = pd.DataFrame(all_stats)
        stats_path = output_dir / "fetch_statistics.csv"
        stats_df.to_csv(stats_path, index=False, encoding=args.encoding)
        
        # æ‰“å°æ±‡æ€»ä¿¡æ¯
        print("\n" + "=" * 60, flush=True)
        print("æ•°æ®è·å–å®Œæˆï¼", flush=True)
        print("=" * 60, flush=True)
        if args.skip_existing:
            print(f"è·³è¿‡å·²ä¸‹è½½: {skipped_count} åª", flush=True)
        print(f"æœ¬æ¬¡ä¸‹è½½: {len(all_stats)} åª", flush=True)
        if failed_stocks:
            print(f"å¤±è´¥: {len(failed_stocks)} åª", flush=True)
        print(f"æ€»è‚¡ç¥¨æ•°: {len(stock_list)} åª", flush=True)
        print(f"æ—¥çº¿æ•°æ®æ€»æ¡æ•°: {stats_df['daily'].sum()}", flush=True)
        print(f"å‘¨çº¿æ•°æ®æ€»æ¡æ•°: {stats_df['weekly'].sum()}", flush=True)
        print(f"æœˆçº¿æ•°æ®æ€»æ¡æ•°: {stats_df['monthly'].sum()}", flush=True)
        print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}", flush=True)
        
        # å¦‚æœæœ‰å¤±è´¥çš„è‚¡ç¥¨ï¼Œä¿å­˜åˆ°æ–‡ä»¶
        if failed_stocks:
            failed_path = output_dir / "failed_stocks.txt"
            with open(failed_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(failed_stocks))
            print(f"å¤±è´¥è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ°: {failed_path}", flush=True)
        
        print("=" * 60, flush=True)
        
    except Exception as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        return 1
    finally:
        logout()
    
    return 0


if __name__ == "__main__":
    raise SystemExit(main())

